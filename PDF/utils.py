import random
from collections import defaultdict

import numpy as np
from keras import backend as K
from keras.models import Model
from configs import feature_constraints


def normalize(x):
    # utility function to normalize a tensor by its L2 norm
    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)


def features_changed(gen_pdf, orig_pdf, feat_names):
    ret_str = ''
    changes = gen_pdf[0] - orig_pdf[0]
    non_zero_changes_idx = np.nonzero(changes)
    feat_names = np.array(feat_names)
    for name, change in zip(feat_names[non_zero_changes_idx], changes[non_zero_changes_idx]):
        ret_str += '{}: {} '.format(name, change)
    return ret_str


def init_feature_constraints(feat_names):
    incre_idx = [feat_names.index(incre_feats) for incre_feats in feature_constraints.increment]
    incre_decre_idx = [feat_names.index(incre_decre_feats) for incre_decre_feats in feature_constraints.incre_decre]
    return incre_idx, incre_decre_idx


def constraint(gradients, incre_idx, incre_decre_idx):
    new_grads = np.zeros_like(gradients)
    new_grads[..., incre_decre_idx] = gradients[:, incre_decre_idx]

    # make gradients to be all positive, and get the indices of features that can only be increased
    positive_grad = gradients.copy()
    positive_grad[positive_grad < 0] = 0
    new_grads[..., incre_idx] = positive_grad[:, incre_idx]
    return new_grads


def init_coverage_tables(model1, model2, model3):
    model_layer_dict1 = defaultdict(bool)
    model_layer_dict2 = defaultdict(bool)
    model_layer_dict3 = defaultdict(bool)
    init_dict(model1, model_layer_dict1)
    init_dict(model2, model_layer_dict2)
    init_dict(model3, model_layer_dict3)
    return model_layer_dict1, model_layer_dict2, model_layer_dict3


def init_dict(model, model_layer_dict):
    for layer in model.layers:
        if 'flatten' in layer.name or 'input' in layer.name:
            continue
        for index in range(num_neurons(layer.output_shape)): # product of dims
            model_layer_dict[(layer.name, index)] = False


def neuron_to_cover(model_layer_dict):
    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_dict.items() if not v]
    if not_covered:
        layer_name, index = random.choice(not_covered)
    else:
        layer_name, index = random.choice(model_layer_dict.keys())
    return layer_name, index


def neuron_covered(model_layer_dict):
    covered_neurons = len([v for v in model_layer_dict.values() if v])
    total_neurons = len(model_layer_dict)
    return covered_neurons, total_neurons, covered_neurons / float(total_neurons)


def scale(intermediate_layer_output, rmax=1, rmin=0):
    X_std = (intermediate_layer_output - intermediate_layer_output.min()) / (
        intermediate_layer_output.max() - intermediate_layer_output.min())
    X_scaled = X_std * (rmax - rmin) + rmin
    return X_scaled


def update_coverage(input_data, model, model_layer_dict, model_layer_hl_dict, threshold=0):
    layer_names = [layer.name for layer in model.layers if
                   'flatten' not in layer.name and 'input' not in layer.name]

    intermediate_layer_model = Model(inputs=model.input,
                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])
    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)

    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):
        layer = intermediate_layer_output[0]
        for neuron in xrange(num_neurons(layer.shape)): # index through every single (indiv) neuron
            _,high = model_layer_hl_dict[(layer_names[i], neuron)]
            if  layer[np.unravel_index(neuron, layer.shape)] > high and not model_layer_dict[(layer_names[i], neuron)]: # get rid of mean
                model_layer_dict[(layer_names[i], neuron)] = True

def num_neurons(shape):
    return reduce(lambda x,y: x*y, filter(lambda x : x != None, shape))

def full_coverage(model_layer_dict):
    if False in model_layer_dict.values():
        return False
    return True


def fired(model, layer_name, index, input_data, threshold=0):
    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)
    intermediate_layer_output = intermediate_layer_model.predict(input_data)[0]
    scaled = scale(intermediate_layer_output)
    if np.mean(scaled[..., index]) > threshold:
        return True
    return False


def diverged(predictions1, predictions2, predictions3, target):
    #     if predictions2 == predictions3 == target and predictions1 != target:
    if not predictions1 == predictions2 == predictions3:
        return True
    return False
